{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><i> Big Data Project </i> </center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center><b> Twitter sentimental analysis and classification of tweets </b> </center>"
   ]
  },
  
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem Statement and Introduction \n",
    "\n",
    "In this project, we are building a model to that analyses a stream of tweets on twitter and displays the sentiment of the tweet. The model is trained and tested using the given csv data. We renamed training csv file to trainingdata.csv to make it readable.The data set includes 1,600,000 tweets. We have used hadoop spark on jupyter notebook environment to build the model. The model is built using logistic regression library and the output obtained is stored in MsSQL database. \n",
    "\n",
    "The entire project is divided into 3 main parts : \n",
    "\n",
    "1) Building a classifier model using the given training and test data \n",
    "\n",
    "2) Using the model that is built in part1 to classify the Tweets \n",
    "\n",
    "3) Storing the data into database - MsSQL using Python module- pyodbc that makes accessing of ODBC database simple. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 1\n",
    "### Building classifier model "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Loading the required packages </b>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init('C:\\Users\\samir\\Desktop\\spark\\spark-2.4.7-bin-hadoop2.7')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col\n",
    "from pyspark.ml.feature import HashingTF, Tokenizer, StopWordsRemover"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Creating a spark session with the user defined appName tweetsentimentalanalysis. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName('tweetsentimentanalysis').getOrCreate()\n",
    "sc =spark.sparkContext\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read data file into Spark dataFrameÂ¶"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The given training data is used to train the model. Training data consists of 6 fields. Polarity which indicates the sentiment of the tweet( 4- positive, 0- negative,2-neutral),ID represents unique identification number of the tweet, Date of the tweet posted, Query (if there is no query, field has NO_QUERY), User- Name of the user who posted the tweet and Text -the actual tweet. Training data given is in CSV format. This data is loaded into the  DataFrame and schema of the dataframe is printed out in the below code block to get proper visualization of our data information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Polarity: integer (nullable = true)\n",
      " |-- ID: long (nullable = true)\n",
      " |-- Date: string (nullable = true)\n",
      " |-- Query: string (nullable = true)\n",
      " |-- User: string (nullable = true)\n",
      " |-- Text: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tweettraindata = spark.read.csv('trainingdata.csv',inferSchema='true').toDF(\"Polarity\", \"ID\", \"Date\", \"Query\",\"User\",\"Text\")\n",
    "\n",
    "tweettraindata.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> A sample of 10 rows of data in the dataframe is shown below  </b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----------+--------------------+--------+---------------+--------------------+\n",
      "|Polarity|        ID|                Date|   Query|           User|                Text|\n",
      "+--------+----------+--------------------+--------+---------------+--------------------+\n",
      "|       0|1467810369|Mon Apr 06 22:19:...|NO_QUERY|_TheSpecialOne_|@switchfoot http:...|\n",
      "|       0|1467810672|Mon Apr 06 22:19:...|NO_QUERY|  scotthamilton|is upset that he ...|\n",
      "|       0|1467810917|Mon Apr 06 22:19:...|NO_QUERY|       mattycus|@Kenichan I dived...|\n",
      "|       0|1467811184|Mon Apr 06 22:19:...|NO_QUERY|        ElleCTF|my whole body fee...|\n",
      "|       0|1467811193|Mon Apr 06 22:19:...|NO_QUERY|         Karoli|@nationwideclass ...|\n",
      "|       0|1467811372|Mon Apr 06 22:20:...|NO_QUERY|       joy_wolf|@Kwesidei not the...|\n",
      "|       0|1467811592|Mon Apr 06 22:20:...|NO_QUERY|        mybirch|         Need a hug |\n",
      "|       0|1467811594|Mon Apr 06 22:20:...|NO_QUERY|           coZZ|@LOLTrish hey  lo...|\n",
      "|       0|1467811795|Mon Apr 06 22:20:...|NO_QUERY|2Hood4Hollywood|@Tatiana_K nope t...|\n",
      "|       0|1467812025|Mon Apr 06 22:20:...|NO_QUERY|        mimismo|@twittera que me ...|\n",
      "+--------+----------+--------------------+--------+---------------+--------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tweettraindata.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Extraction "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Out of the six fields present in the dataframe we need only 2 fields for our model - tweet text and Polarity. These fields are selected and sample of 5 rows are printed below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------------------------------------------------------------------------------------------------+--------+\n",
      "|Text                                                                                                               |Polarity|\n",
      "+-------------------------------------------------------------------------------------------------------------------+--------+\n",
      "|@switchfoot http://twitpic.com/2y1zl - Awww, that's a bummer.  You shoulda got David Carr of Third Day to do it. ;D|0       |\n",
      "|is upset that he can't update his Facebook by texting it... and might cry as a result  School today also. Blah!    |0       |\n",
      "|@Kenichan I dived many times for the ball. Managed to save 50%  The rest go out of bounds                          |0       |\n",
      "|my whole body feels itchy and like its on fire                                                                     |0       |\n",
      "|@nationwideclass no, it's not behaving at all. i'm mad. why am i here? because I can't see you all over there.     |0       |\n",
      "+-------------------------------------------------------------------------------------------------------------------+--------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "selected_data= tweettraindata.select(\"Text\", col(\"Polarity\").cast(\"Int\").alias(\"Polarity\"))\n",
    "selected_data.show(truncate=False,n=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train and Test data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data set is divided into train and test data using ramdomSplit function. 70% of the data is divided into training data and rest 30% is used as test data. Number of row in training data and test data is shown below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Training data rows:', 1120149, '; Testing data rows:', 479851)\n"
     ]
    }
   ],
   "source": [
    "#divide dataset as 70% training and 30%test\n",
    "dividedData = selected_data.randomSplit([0.70,0.30])\n",
    "trainingData = dividedData[0] #index 0 = data training\n",
    "testingData = dividedData[1] #index 1 = data testing\n",
    "train_rows = trainingData.count()\n",
    "test_rows = testingData.count()\n",
    "print (\"Training data rows:\", train_rows, \"; Testing data rows:\", test_rows)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing data\n",
    "### Tokenization:\n",
    "We did Tokenization as first preprocessing technique. It is the method of separating and classifying parts of string in the sentence.The text column in the training data frame consists of sentences. Each of the sentences are broken down to words and a list of all the words of the tweet is stored in a new column under the name : SeWords. This is done using the Tokenizer module as shown in the code block below.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------------------------------------------------------------------------------+--------+-------------------------------------------------------------------------------------------------------------------+\n",
      "|Text                                                                                         |Polarity|SeWords                                                                                                            |\n",
      "+---------------------------------------------------------------------------------------------+--------+-------------------------------------------------------------------------------------------------------------------+\n",
      "|       i really2 don't like this condition. sucksssssss                                      |0       |[, , , , , , , i, really2, don't, like, this, condition., sucksssssss]                                             |\n",
      "|      My current headset is on its deathbed now!  My dad gave it to me just 3 weeks back!    |0       |[, , , , , , my, current, headset, is, on, its, deathbed, now!, , my, dad, gave, it, to, me, just, 3, weeks, back!]|\n",
      "|      this weekend has sucked so far                                                         |0       |[, , , , , , this, weekend, has, sucked, so, far]                                                                  |\n",
      "|     &lt;- but mustache man is not that desperate                                            |0       |[, , , , , &lt;-, but, mustache, man, is, not, that, desperate]                                                    |\n",
      "|     I dont like this weekend.. Huhuhu ( (                                                   |0       |[, , , , , i, dont, like, this, weekend.., huhuhu, (, (]                                                           |\n",
      "+---------------------------------------------------------------------------------------------+--------+-------------------------------------------------------------------------------------------------------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tokenizer = Tokenizer(inputCol=\"Text\", outputCol=\"SeWords\")\n",
    "tokenizedTrain = tokenizer.transform(trainingData)\n",
    "tokenizedTrain.show(truncate=False, n=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing unwanted information"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the main ways of preprocessing is to filter out unnecessary data.SeWords column consists of list of all the words in the tweet. For effective analysis of the sentiment of the tweet, we need to remove some words that are not necessary. We call it stopwords. So, we get rid of those stopwordsusing the StopWordsRemover. StopWordsRemover is a Transformer takes a String array of words and returns a String array after removing all the defined stop words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------------------------------------------------------------------------------+--------+-------------------------------------------------------------------------------------------------------------------+----------------------------------------------------------------------------+\n",
      "|Text                                                                                         |Polarity|SeWords                                                                                                            |MeaningfulWords                                                             |\n",
      "+---------------------------------------------------------------------------------------------+--------+-------------------------------------------------------------------------------------------------------------------+----------------------------------------------------------------------------+\n",
      "|       i really2 don't like this condition. sucksssssss                                      |0       |[, , , , , , , i, really2, don't, like, this, condition., sucksssssss]                                             |[, , , , , , , really2, like, condition., sucksssssss]                      |\n",
      "|      My current headset is on its deathbed now!  My dad gave it to me just 3 weeks back!    |0       |[, , , , , , my, current, headset, is, on, its, deathbed, now!, , my, dad, gave, it, to, me, just, 3, weeks, back!]|[, , , , , , current, headset, deathbed, now!, , dad, gave, 3, weeks, back!]|\n",
      "|      this weekend has sucked so far                                                         |0       |[, , , , , , this, weekend, has, sucked, so, far]                                                                  |[, , , , , , weekend, sucked, far]                                          |\n",
      "|     &lt;- but mustache man is not that desperate                                            |0       |[, , , , , &lt;-, but, mustache, man, is, not, that, desperate]                                                    |[, , , , , &lt;-, mustache, man, desperate]                                 |\n",
      "|     I dont like this weekend.. Huhuhu ( (                                                   |0       |[, , , , , i, dont, like, this, weekend.., huhuhu, (, (]                                                           |[, , , , , dont, like, weekend.., huhuhu, (, (]                             |\n",
      "+---------------------------------------------------------------------------------------------+--------+-------------------------------------------------------------------------------------------------------------------+----------------------------------------------------------------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "swr = StopWordsRemover(inputCol=tokenizer.getOutputCol(), \n",
    "                       outputCol=\"MeaningfulWords\")\n",
    "SwRemovedTrain = swr.transform(tokenizedTrain)\n",
    "SwRemovedTrain.show(truncate=False, n=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Numerical features are created from Meaningful words using code below. HashingTF funtion using Austin Appleby's MurmurHash 3 algorithm is implemented. Sample output of top 3 rows are displayed after implementing the code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----------------------------------------------------------------------------+------------------------------------------------------------------------------------------------------------------------+\n",
      "|Polarity|MeaningfulWords                                                             |features                                                                                                                |\n",
      "+--------+----------------------------------------------------------------------------+------------------------------------------------------------------------------------------------------------------------+\n",
      "|0       |[, , , , , , , really2, like, condition., sucksssssss]                      |(262144,[9346,20263,157492,208258,249180],[1.0,1.0,1.0,1.0,7.0])                                                        |\n",
      "|0       |[, , , , , , current, headset, deathbed, now!, , dad, gave, 3, weeks, back!]|(262144,[89074,92854,107144,114629,132612,133824,153489,233502,233677,249180],[1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,7.0])|\n",
      "|0       |[, , , , , , weekend, sucked, far]                                          |(262144,[148694,166162,189170,249180],[1.0,1.0,1.0,6.0])                                                                |\n",
      "+--------+----------------------------------------------------------------------------+------------------------------------------------------------------------------------------------------------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "hashTF = HashingTF(inputCol=swr.getOutputCol(), outputCol=\"features\")\n",
    "numericTrainData = hashTF.transform(SwRemovedTrain).select(\n",
    "    'Polarity', 'MeaningfulWords', 'features')\n",
    "numericTrainData.show(truncate=False, n=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the model\n",
    "\n",
    "To train out classifier model we use Logistic Regression. The LogisticRegression library is imported form pysark and training data frame columns features and polarity are passed as inputs with the maximum iteration equal to 10. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training is done!\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.classification import LogisticRegression\n",
    "\n",
    "lr = LogisticRegression(labelCol=\"Polarity\", featuresCol=\"features\", \n",
    "                        maxIter=10, regParam=0.01)\n",
    "model = lr.fit(numericTrainData)\n",
    "print (\"Training is done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare testing data\n",
    "\n",
    "Test data is prepared similar to training data. Each tweet text is divided into a list of words and the unwanted words are removed. Features of the meaningful words are obtained using hashTF function. A sample top 2 rows of test dataframe is shown below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---------------------------------------------------------------------+----------------------------------------------------------------------------------------+\n",
      "|Polarity|MeaningfulWords                                                      |features                                                                                |\n",
      "+--------+---------------------------------------------------------------------+----------------------------------------------------------------------------------------+\n",
      "|0       |[, , , , , , , , , , fuck, you!]                                     |(262144,[164046,237111,249180],[1.0,1.0,10.0])                                          |\n",
      "|0       |[, , , , , , , , , , want, ben&amp;jerrys, cake, batter, please, ugh]|(262144,[13007,56397,137422,190256,230921,249180,252290],[1.0,1.0,1.0,1.0,1.0,10.0,1.0])|\n",
      "+--------+---------------------------------------------------------------------+----------------------------------------------------------------------------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tokenizedTest = tokenizer.transform(testingData)\n",
    "SwRemovedTest = swr.transform(tokenizedTest)\n",
    "numericTest = hashTF.transform(SwRemovedTest).select(\n",
    "    'Polarity', 'MeaningfulWords', 'features')\n",
    "numericTest.show(truncate=False, n=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict testing data and calculate the accuracy model\n",
    "\n",
    "Test data is passed into the built model and and the prediction made by the model is stored under the column name 'Prediction'. \n",
    "The accuracy of the model is caculated based on the test data results using the prediction and input data. Accuracy is obtained by dividing (the number of correct predictions made by model) / (total number of predictions made by the model). We have obtained an accuracy of 72.5 % as shown below in the output. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------------------------------------------------------------------------------+----------+--------+\n",
      "|MeaningfulWords                                                                                     |prediction|Polarity|\n",
      "+----------------------------------------------------------------------------------------------------+----------+--------+\n",
      "|[, , , , , , , , , , fuck, you!]                                                                    |0.0       |0       |\n",
      "|[, , , , , , , , , , want, ben&amp;jerrys, cake, batter, please, ugh]                               |0.0       |0       |\n",
      "|[, , , , , , , , head, feels, like, bowling, ball]                                                  |0.0       |0       |\n",
      "|[, , , , , #canucks]                                                                                |0.0       |0       |\n",
      "|[, , , , , jb, isnt, showing, australia, more!]                                                     |0.0       |0       |\n",
      "|[, , , , , fucccckkkkkkkkkk]                                                                        |0.0       |0       |\n",
      "|[, , , , cut, beard, off., growing, well, year., gonna, start, over., @shaunamanu, happy, meantime.]|0.0       |0       |\n",
      "|[, , , , wompppp, wompp]                                                                            |0.0       |0       |\n",
      "|[, , , *old, me's, dead, gone*]                                                                     |4.0       |0       |\n",
      "|[, , , boring, , , ):, whats, wrong, him??, , , , , please, tell, me........, , , :-/]              |0.0       |0       |\n",
      "+----------------------------------------------------------------------------------------------------+----------+--------+\n",
      "only showing top 10 rows\n",
      "\n",
      "('correct prediction:', 347786, 'total data:', 479851, 'accuracy:', 0.7247791501945395)\n"
     ]
    }
   ],
   "source": [
    "prediction = model.transform(numericTest)\n",
    "\n",
    "predictionFinal = prediction.select(\n",
    "    'MeaningfulWords', 'prediction', 'Polarity')\n",
    "predictionFinal.show(n=10, truncate = False)\n",
    "\n",
    "correctPrediction = predictionFinal.filter(\n",
    "    predictionFinal['prediction'] == predictionFinal['Polarity']).count()\n",
    "totalData = predictionFinal.count()\n",
    "accuracy = float(correctPrediction)/float(totalData)\n",
    "accuracy\n",
    "print('correct prediction:', correctPrediction, 'total data:', totalData, \n",
    "      'accuracy:', accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model built is saved as 'comodel' and is used in step2 of the project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('comodel')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
